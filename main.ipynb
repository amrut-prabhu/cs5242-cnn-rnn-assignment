{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "**ASSIGNMENT DEADLINE: 4 Mar 2019 (MON) 17:00**\n",
    "\n",
    "In this assignemnt, the task is to implement some basic components for training convolutional neural network over the MNIST dataset. Firstly, you need to implement the adam algorithm in nn/optimizers.py. Secondly, you need to implement the operations in nn/operations.py, which are used by the layers (in nn/layer.py). Finally, you need to tune the model structure and some hyperparameters to improve the accuracy.\n",
    "\n",
    "**Attention**:\n",
    "- *Only python3 is allowed to use in this assignment.*\n",
    "- *`numpy` is utilized for computation.*\n",
    "- *You do not need a GPU to for this assignment. CPU is enough.*\n",
    "- *To run this Jupyter notebook, you need to install the depedent libraries as stated in [README.MD](README.MD).*\n",
    "- *Please do not run this whole file before you implement all the codes. Otherwise some errors will occur.*\n",
    "- *Please do not change the inputs values of the functions needing to be implemented, otherwise your implementation may be wrong tested by our codes.*\n",
    "- *After you implement one function, remember to restart the notebook kernel to help it recognize your fresh code.*\n",
    "- *Please do not change the structure of files in the whole folder of this assignment, otherwise TA may mark your code wrongly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Structure of Codes\n",
    "\n",
    "The structure of provided codes and the functionality of its containing files is shown as below:\n",
    "\n",
    "```bash\n",
    "codes/\n",
    "    data/\n",
    "        mnist.npz       # mnist dataset \n",
    "    models/    # example models of your tiny deep learning framework\n",
    "        MNISTNet.py     # example model on MNIST dataset\n",
    "    nn/        # components of neural networks\n",
    "        layers.py       # layer abstract for CNN\n",
    "        loss.py         # loss function for optimization\n",
    "        model.py        # model abstraction for defining and training models\n",
    "        operations.py   # operation abstraction for defined layers, your main workspace\n",
    "        optimizers.py   # optimizing methods, you only need to implement Adam\n",
    "    utils/     # some additional tools for CNN\n",
    "        check_grads.py  # help you check whether your forward function and backward function are consistent\n",
    "        datasets.py     # load dataset, like MNIST\n",
    "        initializers.py # initializing methods to initialize parameters (like weights, bias)\n",
    "        tools.py        # other useful functions\n",
    "    main.ipynb # interactive notebook, help you understand your task\n",
    "    README.MD  # requirements to run main.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Functionality of This Notebook\n",
    "\n",
    "This iPython notebook serves to:\n",
    "\n",
    "- explain code structure, main APIs and implementation examples (`FC`) \n",
    "- explain your task\n",
    "- provide code to test your implemented forward and backward function for different operations\n",
    "- provide related materials to help you understand the implementation of some operations and optimizers\n",
    "\n",
    "*You can type `jupyter lab` in the terminal to start this jupyter notebook while you are in the folder containing this file. It's much more convinient than jupyter notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Explanation on Abstraction\n",
    "\n",
    "In [nn/optimizers.py](nn/optimizers.py), optimizers (like Adam) you need to implement are inherited from class `Optimizer`. It's defined as:\n",
    "\n",
    "```python\n",
    "class Optimizer():\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        \"\"\"Initialization\n",
    "\n",
    "        # Arguments\n",
    "            lr: float, learnig rate \n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, x, x_grad, iteration):\n",
    "        \"\"\"Update parameters with gradients\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sheduler(self, func, iteration):\n",
    "        \"\"\"learning rate sheduler, to change learning rate with respect to iteration\n",
    "\n",
    "        # Arguments\n",
    "            func: function, arguments are lr and iteration\n",
    "            iteration: int, current iteration number in the whole training process (not in that epoch)\n",
    "\n",
    "        # Returns\n",
    "            lr: float, the new learning rate\n",
    "        \"\"\"\n",
    "        lr = func(self.lr, iteration)\n",
    "        return lr\n",
    "```\n",
    "\n",
    "We have implemented SGD, Adagrad, RMSprop for you. **You only need to implement the Adam optimizer in the [nn/optimizers.py](nn/optimizers.py) following the same style.**\n",
    "\n",
    "In your main workspace [nn/operations.py](nn/operations.py), operations (like conv, pool) you need to implement are inherited from class `operation`. The `operation` class is defined as below:\n",
    "\n",
    "```python\n",
    "class operation(object):\n",
    "    \"\"\"\n",
    "    Operation abstraction\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward operation, reture output\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        \"\"\"Backward operation, return gradient to input\"\"\"\n",
    "        raise NotImplementedError\n",
    "```\n",
    "\n",
    "We have implemented some operations (like relu, fc, flatten) for you. **You only need to implement the rest operations in the [nn/operations.py](nn/operations.py) following the same style.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer\n",
    "In the file [nn/optimizers.py](nn/optimizers.py), there are 4 types of optimizer (`SGD`, `Adam`, `RMSprop` and `Adagrad`). **You only need to implement the `update` function of `Adam`**. \n",
    "\n",
    "`Adam` optimizer is initialized like this:\n",
    "\n",
    "```python\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, sheduler_func=None):\n",
    "        super(Adam, self).__init__(lr)\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        if not self.epsilon:\n",
    "            self.epsilon = 1e-8\n",
    "        self.moments = None\n",
    "        self.accumulators = None\n",
    "        self.sheduler_func = sheduler_func\n",
    "```\n",
    "\n",
    "- `lr`: The initial learning rate.\n",
    "- `decay`: The learning rate decay ratio\n",
    "- `sheduler_func`: Function to change learning rate with respect to iterations\n",
    "\n",
    "More details can be seen in the reference.\n",
    "\n",
    "*For you reference:* http://cs231n.github.io/neural-networks-3/#update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covolution Layer\n",
    "In the file [nn/layers.py](nn/layers.py), the initialization, forward and backward function of the class `Convolution` are shown as below:\n",
    "```python\n",
    "class Convolution(Layer):\n",
    "    def __init__(self, conv_params, initializer=Guassian(), name='conv'):\n",
    "        super(Convolution, self).__init__(name=name)\n",
    "        self.trainable = True\n",
    "        self.conv_params = conv_params\n",
    "        self.conv = conv(conv_params)\n",
    "\n",
    "        self.weights = initializer.initialize(\n",
    "            (conv_params['out_channel'], conv_params['in_channel'], conv_params['kernel_h'], conv_params['kernel_w']))\n",
    "        self.bias = np.zeros((conv_params['out_channel']))\n",
    "\n",
    "        self.w_grad = np.zeros(self.weights.shape)\n",
    "        self.b_grad = np.zeros(self.bias.shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv.forward(input, self.weights, self.bias)\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        in_grad, self.w_grad, self.b_grad = self.conv.backward(\n",
    "            out_grad, input, self.weights, self.bias)\n",
    "        return in_grad\n",
    "\n",
    "```\n",
    "\n",
    "`conv_params` is a dictionary, containing these parameters:\n",
    "\n",
    "- 'kernel_h': The height of kernel.\n",
    "- 'kernel_w': The width of kernel.\n",
    "- 'stride': The number of pixels between adjacent receptive fields in the horizontal and vertical directions.\n",
    "- 'pad': The number of pixels padded to the bottom, top, left and right of each feature map. **Here pad=2 means adding 2 zeros to the left, right, top and bottom respectively**.\n",
    "- 'in_channel': The number of input channels.\n",
    "- 'out_channel': The number of output channels.\n",
    "\n",
    "`initializer` is an instance of Initializer class, used to initialize parameters\n",
    "\n",
    "You need to implement forward and backward funtion of `conv` class in [nn/operations.py](nn/operations.py), which are called in the `Convolution` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward\n",
    "In the file [nn/operations.py](nn/operations.py), implement the forward function for `conv` class.\n",
    "\n",
    "The input consists of N data points, each with C channels, height H and width W. We convolve each input with K different kernels, where each filter spans all C channels and has height HH and width WW.\n",
    "\n",
    "**WARNING: Please implement the matrix product method of convolution as shown in Lecture notes. The naive version of implementing a sliding window will be too slow when you try to train the whole CNN in later sections.**\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error (<1e-6 will be fine):  3.4611371076604895e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nn.layers import Convolution\n",
    "from utils.tools import rel_error\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "input = np.random.uniform(size=(10, 3, 30, 30))\n",
    "params = { \n",
    "    'kernel_h': 5,\n",
    "    'kernel_w': 5,\n",
    "    'pad': 0,\n",
    "    'stride': 2,\n",
    "    'in_channel': input.shape[1],\n",
    "    'out_channel': 64,\n",
    "}\n",
    "conv = Convolution(params)\n",
    "out = conv.forward(input)\n",
    "\n",
    "keras_conv = Sequential([\n",
    "    Conv2D(filters=params['out_channel'],\n",
    "            kernel_size=(params['kernel_h'], params['kernel_w']),\n",
    "            strides=(params['stride'], params['stride']),\n",
    "            padding='valid',\n",
    "            data_format='channels_first',\n",
    "            input_shape=input.shape[1:]),\n",
    "])\n",
    "keras_conv.layers[0].set_weights([conv.weights.transpose((2,3,1,0)), conv.bias])\n",
    "\n",
    "keras_out = keras_conv.predict(input, batch_size=input.shape[0])\n",
    "print('Relative error (<1e-6 will be fine): ', rel_error(out, keras_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward\n",
    "Implement the backward function for the `conv` class in the file [nn/operations.py](nn/operations.py). \n",
    "\n",
    "When you are done, restart jupyter notebook and run the following to check your backward pass with a numeric gradient check. \n",
    "\n",
    "In gradient checking, to get an approximate gradient for a parameter, we vary that parameter by a small amount (while keeping rest of parameters constant) and note the difference in the network loss. Dividing the difference in network loss by the amount we varied the parameter gives us an approximation for the gradient. We repeat this process for all the other parameters to obtain our numerical gradient. Note that gradient checking is a slow process (2 forward propagations per parameter) and should only be used to check your backpropagation!\n",
    "\n",
    "More links on gradient checking:\n",
    "\n",
    "http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient to input: correct\n",
      "5.309135897513092e-13 1.2324775819274949e-12 1.486596798715307e-11\n",
      "Gradient to weights:  correct\n",
      "Gradient to bias:  correct\n"
     ]
    }
   ],
   "source": [
    "from nn.layers import Convolution\n",
    "import numpy as np\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "batch = 10\n",
    "conv_params={\n",
    "    'kernel_h': 3,\n",
    "    'kernel_w': 3,\n",
    "    'pad': 0,\n",
    "    'stride': 2,\n",
    "    'in_channel': 3,\n",
    "    'out_channel': 10\n",
    "}\n",
    "in_height = 10\n",
    "in_width = 20\n",
    "out_height = 1+(in_height+2*conv_params['pad']-conv_params['kernel_h'])//conv_params['stride']\n",
    "out_width = 1+(in_width+2*conv_params['pad']-conv_params['kernel_w'])//conv_params['stride']\n",
    "\n",
    "input = np.random.uniform(size=(batch, conv_params['in_channel'], in_height, in_width))\n",
    "out_grad = np.random.uniform(size=(batch, conv_params['out_channel'], out_height, out_width))\n",
    "\n",
    "conv = Convolution(conv_params)\n",
    "check_grads_layer(conv, input, out_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "In the file [nn/layers.py](nn/layers.py), the initialization, forward and backward funtion of the class `Pooling` are shown as below:\n",
    "\n",
    "```python\n",
    "class Pooling(Layer):\n",
    "    def __init__(self, pool_params, name='pooling'):\n",
    "        super(Pooling, self).__init__(name=name)\n",
    "        self.pool_params = pool_params\n",
    "        self.pool = pool(pool_params)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.pool.forward(input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        in_grad = self.pool.backward(out_grad, input)\n",
    "        return in_grad\n",
    "```\n",
    "\n",
    "`pool_params` is a dictionary, containing these parameters:\n",
    "- 'pool_type': The type of pooling, 'max' or 'avg'\n",
    "- 'pool_h': The height of pooling kernel.\n",
    "- 'pool_w': The width of pooling kernel.\n",
    "- 'stride': The number of pixels between adjacent receptive fields in the horizontal and vertical directions.\n",
    "- 'pad': The number of pixels that will be used to zero-pad the input in each x-y direction. **Here pad=2 means adding 2 zeros to the left, right, top and bottom respectively**.\n",
    "\n",
    "You need to implement forward and backward funtion of `pool` class in [nn/operations.py](nn/operations.py), which are called in `Pooling` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward\n",
    "Implement the forward function for `pool` class in the file [nn/operations.py](nn/operations.py).\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nn.layers import Pooling\n",
    "from utils.tools import rel_error\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "input = np.random.uniform(size=(10, 3, 30, 30))\n",
    "params = { \n",
    "    'pool_type': 'max',\n",
    "    'pool_height': 5,\n",
    "    'pool_width': 5,\n",
    "    'pad': 0,\n",
    "    'stride': 2,\n",
    "}\n",
    "pool = Pooling(params)\n",
    "out = pool.forward(input)\n",
    "\n",
    "keras_pool = Sequential([\n",
    "    MaxPooling2D(pool_size=(params['pool_height'], params['pool_width']),\n",
    "                 strides=params['stride'],\n",
    "                 padding='valid',\n",
    "                 data_format='channels_first',\n",
    "                 input_shape=input.shape[1:])\n",
    "])\n",
    "keras_out = keras_pool.predict(input, batch_size=input.shape[0])\n",
    "print('Relative error (<1e-6 will be fine): ', rel_error(out, keras_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward\n",
    "Implement the backward function for `pool` class in the file [nn/operations.py](nn/operations.py). **You need to implement max-pooing and avg-pooling according to 'pool_type' in pool_params**\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient to input: correct\n"
     ]
    }
   ],
   "source": [
    "from nn.layers import Pooling\n",
    "import numpy as np\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "batch = 10\n",
    "pool_params={\n",
    "    'pool_type': 'max',\n",
    "    'pool_height': 2,\n",
    "    'pool_width': 3,\n",
    "    'stride': 2,\n",
    "    'pad': 0\n",
    "}\n",
    "in_height = 10\n",
    "in_width = 10\n",
    "in_channel = 10\n",
    "out_height = 1+(in_height-pool_params['pool_height']+2*pool_params['pad'])//pool_params['stride']\n",
    "out_width = 1+(in_width-pool_params['pool_width']+2*pool_params['pad'])//pool_params['stride']\n",
    "\n",
    "input = np.random.uniform(size=(batch, in_channel, in_height, in_width))\n",
    "out_grads = np.random.uniform(size=(batch, in_channel, out_height, out_width))\n",
    "pool = Pooling(pool_params)\n",
    "check_grads_layer(pool, input, out_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Layer\n",
    "Dropout [1] is a technique for regularizing neural networks by randomly setting some features to zero during the forward pass. In this exercise you will implement a dropout layer and modify your fully-connected network to optionally use dropout.\n",
    "\n",
    "[1] Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012\n",
    "\n",
    "In the file `layers.py`, the initialization, forward and backward function of class `Dropout` are shown as below:\n",
    "\n",
    "```python\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, rate, name='dropout', seed=None):\n",
    "        super(Dropout, self).__init__(name=name)\n",
    "        self.rate = rate\n",
    "        self.seed = seed\n",
    "        self.dropout = dropout(rate, self.training, seed)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.dropout.forward(input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        in_grad = self.dropout.backward(out_grad, input)\n",
    "        return in_grad\n",
    "```\n",
    "\n",
    "- `rate`: The probability of setting a neuron to zero\n",
    "- `seed`: seed: int, random seed to sample from input, so as to get mask, which is convenient to check gradients. But for real training, it should be None to make sure to randomly drop neurons\n",
    "\n",
    "You need to implement forward and backward funtion of `dropout` class in [nn/operations.py](nn/operations.py), which are called in `Dropout` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward\n",
    "In the file [nn/operations.py](nn/operations.py), implement the forward function for `dropout` class. Since dropout behaves differently during training and testing, make sure to implement the operation for both modes.  `p` refers to the probability of setting a neuron to zero. We will follow the Caffe convention where we multiply the outputs by `1/(1-p)` during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward\n",
    "In the file [nn/operations.py](nn/operations.py), implement the backward function for `dropout` class. After the implementation, restart jupyter notebook and run the following cell to numerically gradient-check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient to input: correct\n"
     ]
    }
   ],
   "source": [
    "from nn.layers import Dropout\n",
    "\n",
    "import numpy as np\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "rate = 0.1\n",
    "batch = 2\n",
    "height = 10\n",
    "width = 20\n",
    "channel = 10\n",
    "\n",
    "np.random.seed(1234)\n",
    "input = np.random.uniform(size=(batch, channel, height, width))\n",
    "out_grads = np.random.uniform(size=(batch, channel, height, width))\n",
    "\n",
    "dropout = Dropout(rate, seed=1234)\n",
    "dropout.set_mode(True)\n",
    "check_grads_layer(dropout, input, out_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net on full MNIST data\n",
    "By training the `MNISTNet` for one epoch, you should achieve above 90% on test set. You may have to wait about 5 minutes for training to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  48000\n",
      "Number of validation images:  12000\n",
      "Number of testing images:  10000\n",
      "\n",
      "Four examples of training images:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13a4cea58>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from models.MNISTNet import MNISTNet\n",
    "from nn.loss import SoftmaxCrossEntropy, L2\n",
    "from nn.optimizers import Adam\n",
    "from utils.datasets import MNIST\n",
    "import numpy as np\n",
    "\n",
    "mnist = MNIST()\n",
    "mnist.load()\n",
    "idx = np.random.randint(mnist.num_train, size=4)\n",
    "print('\\nFour examples of training images:')\n",
    "img = mnist.x_train[idx][:,0,:,:]\n",
    "\n",
    "plt.figure(1, figsize=(18, 18))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(img[0])\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(img[1])\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(img[2])\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(img[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: \n",
      "Train iter 100/1600:\tacc 0.10, loss 2.30, reg loss 0.00, speed 1178.28 imgs/sec\n",
      "Train iter 200/1600:\tacc 0.20, loss 2.30, reg loss 0.00, speed 1188.81 imgs/sec\n",
      "Train iter 300/1600:\tacc 0.13, loss 2.30, reg loss 0.00, speed 1193.99 imgs/sec\n",
      "Train iter 400/1600:\tacc 0.17, loss 2.30, reg loss 0.00, speed 1213.00 imgs/sec\n",
      "Test acc 0.11, loss 2.30\n",
      "Train iter 500/1600:\tacc 0.07, loss 2.30, reg loss 0.00, speed 471.32 imgs/sec\n",
      "Train iter 600/1600:\tacc 0.17, loss 2.29, reg loss 0.00, speed 1203.46 imgs/sec\n",
      "Train iter 700/1600:\tacc 0.33, loss 2.20, reg loss 0.00, speed 1201.99 imgs/sec\n",
      "Train iter 800/1600:\tacc 0.30, loss 2.04, reg loss 0.00, speed 1158.76 imgs/sec\n",
      "Train iter 900/1600:\tacc 0.43, loss 1.79, reg loss 0.01, speed 1172.44 imgs/sec\n",
      "Test acc 0.43, loss 1.69\n",
      "Train iter 1000/1600:\tacc 0.40, loss 1.70, reg loss 0.01, speed 476.31 imgs/sec\n",
      "Train iter 1100/1600:\tacc 0.33, loss 1.85, reg loss 0.01, speed 1148.88 imgs/sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-59870aaadb02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtrain_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     val_intervals=-1, test_intervals=500, print_intervals=100)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cost:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/cs5242/nn/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, train_batch, val_batch, test_batch, epochs, val_intervals, test_intervals, print_intervals)\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mreg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/cs5242/nn/model.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, targets)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/cs5242/nn/layers.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, out_grad, input)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         in_grad, self.w_grad, self.b_grad = self.conv.backward(\n\u001b[0;32m--> 145\u001b[0;31m             out_grad, input, self.weights, self.bias)\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0min_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/cs5242/nn/operations.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, out_grad, input, weights, bias)\u001b[0m\n\u001b[1;32m    248\u001b[0m                                  in_height, pad:pad+in_width]\n\u001b[1;32m    249\u001b[0m         w_grad = sum(\n\u001b[0;32m--> 250\u001b[0;31m             list(map(lambda x: np.matmul(x[0], x[1].T), zip(out_grad.reshape(batch, out_channel, -1), input_conv))))\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mw_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/cs5242/nn/operations.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    248\u001b[0m                                  in_height, pad:pad+in_width]\n\u001b[1;32m    249\u001b[0m         w_grad = sum(\n\u001b[0;32m--> 250\u001b[0;31m             list(map(lambda x: np.matmul(x[0], x[1].T), zip(out_grad.reshape(batch, out_channel, -1), input_conv))))\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mw_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nn.optimizers import RMSprop\n",
    "\n",
    "model = MNISTNet()\n",
    "loss = SoftmaxCrossEntropy(num_class=10)\n",
    "\n",
    "# define your learning rate sheduler\n",
    "def func(lr, iteration):\n",
    "    if iteration % 1000 ==0:\n",
    "        return lr*0.5\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "adam = Adam(lr=0.0001, decay=0,  sheduler_func=func, bias_correction=True)\n",
    "l2 = L2(w=0.001) # L2 regularization with lambda=0.001\n",
    "model.compile(optimizer=adam, loss=loss, regularization=l2)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "train_results, val_results, test_results = model.train(\n",
    "    mnist, \n",
    "    train_batch=30, val_batch=1000, test_batch=1000, \n",
    "    epochs=2, \n",
    "    val_intervals=-1, test_intervals=500, print_intervals=100)\n",
    "print('cost:', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(train_results[:,0], train_results[:,1])\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Training accuracy')\n",
    "plt.plot(train_results[:,0], train_results[:,2])\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Testing loss')\n",
    "plt.plot(test_results[:,0], test_results[:, 1])\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('Testing accuracy')\n",
    "plt.plot(test_results[:, 0], test_results[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your best MNISTNet!\n",
    "Tweak the hyperparameters and structure of the MNISTNet. The network is small, hence the training should finish quickly using your CPU (less than 1 hour). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your new model and all training codes here, like loading data, defining optimizer and so on\n",
    "\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Description and Analysis\n",
    "Please write down what you have tuned for your best MNISTNet. And please write down your analysis about their impacts on performance.\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "write here (remove it before writing your description and analysis)\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marking Scheme\n",
    "\n",
    "Marking scheme is shown below:\n",
    "- 4 marks for `Adam` update function\n",
    "- 5 marks for `conv` forward and backward function\n",
    "- 4 marks for `pool` forward and backward function\n",
    "- 3 marks for `dropout` forward and backward function\n",
    "- 3 marks for tuning your best MNISTNet\n",
    "- 1 marks for your submission format\n",
    "\n",
    "We will run multiple test cases to check the correctness of the implementation for `Adam`, `conv`, `pool`, `dropout`. As for submission format, please follow below submission instructions.\n",
    "\n",
    "**DO NOT** use external libraries like Tensorflow, keras and Pytorch in your implementation. **DO NOT** copy the code from the internet, e.g. github. We have offered all materials that you can refer to in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final submission instructions\n",
    "Please submit the following:\n",
    "\n",
    "1) Your codes in a folder named `codes`, and keep the structure of all files in this folder the same as what we have provided. \n",
    "\n",
    "**ASSIGNMENT DEADLINE: 4 Mar 2019 (MON) 17:00**\n",
    "\n",
    "Do not include the `data` folder as it takes up substantial memory. Please zip up the following folders under a folder named with your NUSNET ID: eg. `e0123456g.zip` and submit the zipped folder to LumiNUS/Files/Assignment 1 Submission. If unzip the file, the structure should be like this:\n",
    "\n",
    "```bash\n",
    "e0123456g/\n",
    "    codes/\n",
    "        models/\n",
    "            ...\n",
    "        nn/\n",
    "            ...\n",
    "        utils/\n",
    "            ...\n",
    "        main.ipynb\n",
    "        README.MD\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@deathbeds/jupyterlab-fonts": {
   "fontLicenses": {
    "Anonymous Pro Bold": {
     "holders": [
      "Copyright (c) 2009, Mark Simonson (http://www.ms-studio.com, mark@marksimonson.com), with Reserved Font Name Anonymous Pro Minus."
     ],
     "name": "SIL Open Font License 1.1",
     "spdx": "OFL-1.1",
     "text": "Copyright (c) 2009, Mark Simonson (http://www.ms-studio.com, mark@marksimonson.com),\nwith Reserved Font Name Anonymous Pro Minus.\n\nThis Font Software is licensed under the SIL Open Font License, Version 1.1.\nThis license is copied below, and is also available with a FAQ at:\nhttp://scripts.sil.org/OFL\n\n\n-----------------------------------------------------------\nSIL OPEN FONT LICENSE Version 1.1 - 26 February 2007\n-----------------------------------------------------------\n\nPREAMBLE\nThe goals of the Open Font License (OFL) are to stimulate worldwide\ndevelopment of collaborative font projects, to support the font creation\nefforts of academic and linguistic communities, and to provide a free and\nopen framework in which fonts may be shared and improved in partnership\nwith others.\n\nThe OFL allows the licensed fonts to be used, studied, modified and\nredistributed freely as long as they are not sold by themselves. The\nfonts, including any derivative works, can be bundled, embedded,\nredistributed and/or sold with any software provided that any reserved\nnames are not used by derivative works. The fonts and derivatives,\nhowever, cannot be released under any other type of license. The\nrequirement for fonts to remain under this license does not apply\nto any document created using the fonts or their derivatives.\n\nDEFINITIONS\n\"Font Software\" refers to the set of files released by the Copyright\nHolder(s) under this license and clearly marked as such. This may\ninclude source files, build scripts and documentation.\n\n\"Reserved Font Name\" refers to any names specified as such after the\ncopyright statement(s).\n\n\"Original Version\" refers to the collection of Font Software components as\ndistributed by the Copyright Holder(s).\n\n\"Modified Version\" refers to any derivative made by adding to, deleting,\nor substituting -- in part or in whole -- any of the components of the\nOriginal Version, by changing formats or by porting the Font Software to a\nnew environment.\n\n\"Author\" refers to any designer, engineer, programmer, technical\nwriter or other person who contributed to the Font Software.\n\nPERMISSION & CONDITIONS\nPermission is hereby granted, free of charge, to any person obtaining\na copy of the Font Software, to use, study, copy, merge, embed, modify,\nredistribute, and sell modified and unmodified copies of the Font\nSoftware, subject to the following conditions:\n\n1) Neither the Font Software nor any of its individual components,\nin Original or Modified Versions, may be sold by itself.\n\n2) Original or Modified Versions of the Font Software may be bundled,\nredistributed and/or sold with any software, provided that each copy\ncontains the above copyright notice and this license. These can be\nincluded either as stand-alone text files, human-readable headers or\nin the appropriate machine-readable metadata fields within text or\nbinary files as long as those fields can be easily viewed by the user.\n\n3) No Modified Version of the Font Software may use the Reserved Font\nName(s) unless explicit written permission is granted by the corresponding\nCopyright Holder. This restriction only applies to the primary font name as\npresented to the users.\n\n4) The name(s) of the Copyright Holder(s) or the Author(s) of the Font\nSoftware shall not be used to promote, endorse or advertise any\nModified Version, except to acknowledge the contribution(s) of the\nCopyright Holder(s) and the Author(s) or with their explicit written\npermission.\n\n5) The Font Software, modified or unmodified, in part or in whole,\nmust be distributed entirely under this license, and must not be\ndistributed under any other license. The requirement for fonts to\nremain under this license does not apply to any document created\nusing the Font Software.\n\nTERMINATION\nThis license becomes null and void if any of the above conditions are\nnot met.\n\nDISCLAIMER\nTHE FONT SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT\nOF COPYRIGHT, PATENT, TRADEMARK, OR OTHER RIGHT. IN NO EVENT SHALL THE\nCOPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nINCLUDING ANY GENERAL, SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL\nDAMAGES, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF THE USE OR INABILITY TO USE THE FONT SOFTWARE OR FROM\nOTHER DEALINGS IN THE FONT SOFTWARE.\n"
    }
   },
   "fonts": {
    "Anonymous Pro Bold": [
     {
      "fontFamily": "'Anonymous Pro Bold'",
      "src": "url('data:font/woff2;charset=utf-8;base64,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') format('woff2')"
     }
    ]
   },
   "styles": {
    ":root": {
     "--jp-code-font-family": "'Anonymous Pro Bold'",
     "--jp-code-font-size": "16px",
     "--jp-content-font-size1": "16px"
    }
   }
  },
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
